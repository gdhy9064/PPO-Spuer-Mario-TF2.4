{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "uuid": "14ae95f2-e31f-4ed8-81d5-e835842e49a9"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "from gym import Wrapper\n",
    "from gym.spaces import Box\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY\n",
    "import multiprocessing as mp\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Layer\n",
    "tf.compat.v1.disable_eager_execution() # 关闭动态图机制\n",
    "\n",
    "def process_frame(frame, height, width):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    frame = cv2.resize(frame, (width, height))[:, :, None] / 255.\n",
    "    return frame\n",
    "\n",
    "\n",
    "class CustomEnvironment(Wrapper):\n",
    "    def __init__(self, env, height, width):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(low=0, high=1, shape=(height, width, 1))\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        state = process_frame(state, self.height, self.width)\n",
    "        if done:\n",
    "            if info[\"flag_get\"]:\n",
    "                reward += 50\n",
    "            else:\n",
    "                reward -= 50\n",
    "        return state, reward / 10., done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return process_frame(self.env.reset(), self.height, self.width)\n",
    "\n",
    "\n",
    "class SkipFrame(Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(\n",
    "            low=0, \n",
    "            high=1, \n",
    "            shape=(*self.env.observation_space.shape[:-1], skip)\n",
    "        )\n",
    "        self.skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        states = []\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        for i in range(self.skip):\n",
    "            if not done:\n",
    "                state, reward, done, info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                states.append(state)\n",
    "            else:\n",
    "                states.append(state)\n",
    "        states = np.concatenate(states, axis=-1)\n",
    "        return states.astype(np.float32), total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        states = np.concatenate([state for _ in range(self.skip)], axis=-1)\n",
    "        return states.astype(np.float32)\n",
    "\n",
    "    \n",
    "class AutoReset(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        if done:\n",
    "            state = self.env.reset()\n",
    "        return state, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "\n",
    "def create_env(world, stage, action_type, height, width):\n",
    "    env = gym_super_mario_bros.make(f'SuperMarioBros-{world}-{stage}-v0')\n",
    "    env = JoypadSpace(env, action_type)\n",
    "    env = CustomEnvironment(env, height, width)\n",
    "    env = SkipFrame(env)\n",
    "    env = AutoReset(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "class MultipleEnvironments():\n",
    "    def __init__(self, num_envs, create_env, *args):\n",
    "        assert num_envs > 0\n",
    "        self.agent_conns, self.env_conns = zip(*[mp.Pipe() for _ in range(num_envs)])\n",
    "        for conn in self.env_conns:\n",
    "            process = mp.Process(target=self.run, args=(conn, create_env, *args))\n",
    "            process.start()\n",
    "\n",
    "    @staticmethod\n",
    "    def run(conn, create_env, *args):\n",
    "        env = create_env(*args)\n",
    "        while True:\n",
    "            request, action = conn.recv()\n",
    "            if request == 'step':\n",
    "                conn.send(env.step(action))\n",
    "            elif request == 'reset':\n",
    "                conn.send(env.reset())\n",
    "            elif request == 'render':\n",
    "                env.render()\n",
    "            elif request == 'close':\n",
    "                env.close()\n",
    "                break\n",
    "            elif hasattr(env, request):\n",
    "                conn.send(getattr(env, request))\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        if name in self.__dict__:\n",
    "            return self.__dict__[name]\n",
    "        assert not self.agent_conns[0].closed, 'Environment closed.'\n",
    "        self.agent_conns[0].send([name, None])\n",
    "        return self.agent_conns[0].recv()\n",
    "                \n",
    "    def step(self, actions):\n",
    "        assert not self.agent_conns[0].closed, 'Environment closed.'\n",
    "        for conn, action in zip(self.agent_conns, actions):\n",
    "            conn.send(['step', action.item()])\n",
    "        return tuple(zip(*[conn.recv() for conn in self.agent_conns]))\n",
    "    \n",
    "    def reset(self):\n",
    "        assert not self.agent_conns[0].closed, 'Environment closed.'\n",
    "        for conn in self.agent_conns:\n",
    "            conn.send(['reset', None])\n",
    "        return tuple(conn.recv() for conn in self.agent_conns)\n",
    "    \n",
    "    def render(self):\n",
    "        assert not self.agent_conns[0].closed, 'Environment closed.'\n",
    "        for conn in self.agent_conns:\n",
    "            conn.send(['render', None])\n",
    "                \n",
    "    def close(self):\n",
    "        assert not self.agent_conns[0].closed, 'Environment closed.'\n",
    "        for conn in self.agent_conns:\n",
    "            conn.send(['close', None])\n",
    "            conn.close()\n",
    "        for conn in self.env_conns:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "uuid": "4257a487-999c-452c-b51a-b7e0e9b24ca2"
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def store(self, state, action, prob, reward, next_state, done):\n",
    "        ### 逆序插入方便之后计算GAE\n",
    "        self.states.insert(0, state) \n",
    "        self.actions.insert(0, action)\n",
    "        self.probs.insert(0, prob)\n",
    "        self.rewards.insert(0, reward)\n",
    "        self.next_states.insert(0, next_state)\n",
    "        self.dones.insert(0, done)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "\n",
    "        \n",
    "class Feature(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = Sequential([\n",
    "            Conv2D(32, 3, strides=2, activation='relu', padding='same'),\n",
    "            Conv2D(32, 3, strides=2, activation='relu', padding='same'),\n",
    "            Conv2D(32, 3, strides=2, activation='relu', padding='same'),\n",
    "            Conv2D(32, 3, strides=2, activation='relu', padding='same'),\n",
    "            Flatten(),\n",
    "            Dense(512, activation='relu'),\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    \n",
    "class PPOTrainer():\n",
    "    def __init__(\n",
    "        self, \n",
    "        obs_shape, \n",
    "        act_n, \n",
    "        lmbda=0.97, \n",
    "        gamma=0.99, \n",
    "        lr=2e-4, \n",
    "        eps_clip=0.2,\n",
    "        train_step=10,\n",
    "        entropy_coef=0.05,\n",
    "        checkpoint_path='mario',\n",
    "    ):\n",
    "        self.memory = Memory()\n",
    "        self.lmbda = lmbda\n",
    "        self.gamma = gamma \n",
    "        self.lr = lr\n",
    "        self.obs_shape = obs_shape\n",
    "        self.act_n = act_n\n",
    "        self.eps_clip = eps_clip\n",
    "        self.train_step = train_step\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.policy, self.value, self.train_model = self.build_model()\n",
    "        ckpt = tf.train.Checkpoint(\n",
    "            train_model=self.train_model,\n",
    "            optimizer=self.train_model.optimizer,\n",
    "        )\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1) \n",
    "        \n",
    "    def build_model(self):\n",
    "        s_input = Input(self.obs_shape)\n",
    "        prob_old_input = Input([])\n",
    "        action_old_input = Input([], dtype='int32')\n",
    "        gae_input = Input([])\n",
    "        v_target_input = Input([])\n",
    "        \n",
    "        feature = Feature()\n",
    "        x = feature(s_input)\n",
    "        policy_dense = Dense(self.act_n, activation='softmax')\n",
    "        value_dense = Dense(1)\n",
    "        prob = policy_dense(x)\n",
    "        v = value_dense(x)\n",
    "        policy = Model(inputs=s_input, outputs=prob)\n",
    "        value = Model(inputs=s_input, outputs=v)\n",
    "\n",
    "        prob_cur = tf.gather(prob, action_old_input, batch_dims=1)\n",
    "        ratio = prob_cur / (prob_old_input + 1e-3)\n",
    "        surr1 = ratio * gae_input\n",
    "        surr2 = K.clip(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * gae_input\n",
    "        \n",
    "        # 第二项为熵值计算，由于已经按照动作概率采样，因此计算时不再乘上概率，并且只需要计算当前动作概率的对数\n",
    "        policy_loss = -K.mean(K.minimum(surr1, surr2)) + K.mean(K.log(prob_cur + 1e-3)) * self.entropy_coef \n",
    "        \n",
    "        value_loss = K.mean((v[:, 0] - v_target_input) ** 2)\n",
    "        loss = policy_loss + value_loss\n",
    "        train_model = Model(inputs=[s_input, prob_old_input, action_old_input, gae_input, v_target_input], outputs=loss)\n",
    "        train_model.add_loss(loss)\n",
    "        train_model.compile(tf.keras.optimizers.Adam(self.lr))\n",
    "        return policy, value, train_model\n",
    "    \n",
    "    def choose_action(self, states):\n",
    "        # states.shape: (env_num, height, width, skip_frames) \n",
    "        probs = self.policy.predict(states) # shape: (env_num, act_n)\n",
    "        actions = [np.random.choice(range(self.act_n), p=prob) for prob in probs] # shape: (env_num)\n",
    "        return actions, probs[np.arange(len(probs)), actions]\n",
    "\n",
    "    def store(self, states, actions, probs, rewards, next_states, dones):\n",
    "        self.memory.store(states, actions, probs, rewards, next_states, dones)\n",
    "           \n",
    "    def update_model(self, batch_size=128):\n",
    "        states = np.array(self.memory.states) # shape: (-1, env_num, height, width, skip_frames)\n",
    "        actions = np.array(self.memory.actions) # shape: (-1, env_num)\n",
    "        probs = np.array(self.memory.probs) # shape: (-1, env_num)\n",
    "        rewards = np.array(self.memory.rewards) # shape: (-1, env_num)\n",
    "        next_states = np.array(self.memory.next_states) # shape: (-1, env_num, height, width, skip_frames)\n",
    "        dones = np.array(self.memory.dones) # shape: (-1, env_num)\n",
    "        \n",
    "        env_num = states.shape[1]\n",
    "        states = states.reshape([-1, *states.shape[2:]])\n",
    "        next_states = next_states.reshape([-1, *next_states.shape[2:]])\n",
    "        actions = actions.flatten()\n",
    "        probs = probs.flatten()\n",
    "        \n",
    "        for step in range(self.train_step):\n",
    "            v = self.value.predict(states, batch_size=batch_size)\n",
    "            v_next = self.value.predict(next_states, batch_size=batch_size)\n",
    "            v = v.reshape([v.shape[0] // env_num, env_num])\n",
    "            v_next = v_next.reshape([v_next.shape[0] // env_num, env_num])\n",
    "            \n",
    "            v_target = rewards + self.gamma * v_next * ~dones\n",
    "            td_errors = v_target - v\n",
    "            gae_lst = []\n",
    "            adv = 0\n",
    "            for delta in td_errors:\n",
    "                adv = self.gamma * self.lmbda * adv + delta\n",
    "                gae_lst.append(adv)\n",
    "            \n",
    "            gaes = np.array(gae_lst)\n",
    "            gaes = gaes.flatten()\n",
    "            v_target = v_target.flatten()\n",
    "            self.train_model.fit([states, probs, actions, gaes, v_target], batch_size=batch_size)\n",
    "\n",
    "        self.memory.reset()\n",
    "        \n",
    "    def save(self):\n",
    "        self.ckpt_manager.save()\n",
    "        \n",
    "    def load(self):     \n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            status = agent.ckpt_manager.checkpoint.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            status.run_restore_ops() # 关闭动态图后需要添加这句执行restore操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "1c10f9b4-bd3d-44dd-b6a2-e6d1de3bfc3c"
   },
   "outputs": [],
   "source": [
    "session = tf.compat.v1.InteractiveSession() # 关闭动态图后，ckpt_manager.save()需要有默认的session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "uuid": "ed8a66ae-08fd-44f2-a9d9-3e41410c16a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output tf_op_layer_add_7 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_add_7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lib/tensorflow/python/keras/engine/training.py:2342: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | max position: 594 | min position: 331\n",
      "Train on 1024 samples\n",
      "1024/1024 [==============================] - 1s 1ms/sample - loss: -9.1638\n",
      "Train on 1024 samples\n",
      "1024/1024 [==============================] - 1s 981us/sample - loss: -9.1595\n",
      "Train on 1024 samples\n",
      "1024/1024 [==============================] - 1s 1ms/sample - loss: -9.1449\n",
      "Train on 1024 samples\n",
      "1024/1024 [==============================] - 1s 1ms/sample - loss: -9.1143\n",
      "Train on 1024 samples\n",
      "1024/1024 [==============================] - 1s 1ms/sample - loss: -9.0911\n",
      "Train on 1024 samples\n",
      "1024/1024 [==============================] - 1s 990us/sample - loss: -8.9835\n",
      "Train on 1024 samples\n",
      "1024/1024 [==============================] - 1s 969us/sample - loss: -8.8527\n",
      "Train on 1024 samples\n",
      "1024/1024 [==============================] - 1s 1ms/sample - loss: -8.6841\n",
      "Train on 1024 samples\n",
      "1024/1024 [==============================] - 1s 1ms/sample - loss: -8.4269\n",
      "Train on 1024 samples\n",
      "1024/1024 [==============================] - 1s 960us/sample - loss: -8.0426\n"
     ]
    }
   ],
   "source": [
    "max_step = 512\n",
    "num_envs = 8\n",
    "height = 84\n",
    "width = 84\n",
    "world = 1\n",
    "stage = 1\n",
    "action_type = SIMPLE_MOVEMENT\n",
    "try:\n",
    "    env = MultipleEnvironments(num_envs, create_env, world, stage, action_type, height, width)\n",
    "    agent = PPOTrainer(\n",
    "        env.observation_space.shape, \n",
    "        env.action_space.n, \n",
    "        train_step=10, \n",
    "        lr=1e-4, \n",
    "        entropy_coef=0.05, \n",
    "        checkpoint_path=f'mario_{world}_{stage}'\n",
    "    )\n",
    "    agent.load()\n",
    "    states = env.reset()\n",
    "    for epoch in range(1, 201):\n",
    "        max_pos = 0\n",
    "        min_pos = np.inf\n",
    "        for step in range(max_step):\n",
    "            actions, probs = agent.choose_action(np.stack(states, axis=0))\n",
    "            next_states, rewards, dones, infos = env.step(actions)\n",
    "            agent.store(states, actions, probs, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            max_pos = max(max_pos, max([info['x_pos'] for info in infos]))\n",
    "            min_pos = min(min_pos, min([info['x_pos'] if done else np.inf for info, done in zip(infos, dones)]))\n",
    "        clear_output() # jupyter notebook 清屏\n",
    "        print(f'epoch: {epoch} | max position: {max_pos} | min position: {min_pos}')\n",
    "        agent.update_model(batch_size=256)\n",
    "        if epoch % 10 == 0:\n",
    "            agent.save()\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "b95caae2-5cd7-43b0-97a9-8ae9edc2e030"
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
